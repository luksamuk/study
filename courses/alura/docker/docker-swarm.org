#+title: Docker Swarm: Orquestrador de containers

* Dependências

- Docker
- Docker Machine
- VirtualBox

* Revisão de Docker

É possível utilizar o [[https://labs.play-with-docker.com][Play with Docker]] para fazer alguns testes.

* Docker Swarm

Docker Swarm  resolve problemas  de consumo  excessivo de  recursos da  parte de
múltiplos  contêineres, e  também de  KeepAlive (reiniciar  um contêiner  caso o
mesmo seja encerrado de forma inesperada).

Além  disso, o  Swarm  pode ser  usado para  executar  contêineres em  clusters,
executando  um mesmo  conjunto de  contêineres  em várias  máquinas para  evitar
sobrecarga de uma única máquina.

O  Docker Swarm  é um  *orquestrador  de contêineres*,  e define  através de  um
/dispatcher/ a máquina mais adequada para executar um contêiner.

* Usando a Docker Machine

*ATENÇÃO: Segundo o site oficial do Docker, o Docker Machine está depreciado.*

A Docker  Machine cria algumas  máquinas virtuais  leves para testes  com Docker
Swarm. Os  clusters serão criados com  essas máquinas virtuais criadas,  que são
leves e já possuem Docker instalado.

Para visualizar as máquinas ativas:

#+begin_src bash
docker-machine ls
#+end_src

Para criar uma VM:

#+begin_src bash
docker-machine create -d virtualbox vm1
#+end_src

Note que o driver utilizado é o ~virtualbox~.

Podemos também gerenciar o status de execução das VMs:

#+begin_src bash
docker-machine start vm1
docker-machine stop vm1
#+end_src

Finalmente, podemos também acessar uma VM através de SSH:

#+begin_src bash
docker-machine ssh vm1
#+end_src


*** Prováveis problemas

Caso o VirtualBox falhe com ~E_ACCESSDENIED~:

- Veja todas as máquinas criadas com ~docker-machine ls~;
- Remova as desnecessárias com ~docker-machine rm -y <nome>~;
-  Encontre  todos  os  "host-only  ethernet  adapters"  com  ~VBoxManage  list
   hostonlyifs~;
- Remova todos os adapters órfãos com ~VBoxManage hostonlyif remove <nome>~;
- Vá até o diretório ~/etc/vbox~. Se não existir, crie-o;
- Crie um arquivo ~networks.conf~, e adicione a linha a seguir:

   #+begin_src conf
,* 0.0.0.0/0 ::/0
   #+end_src
   
- Crie uma nova máquina normalmente;
- Execute o comando ~eval $(docker-machine env <nome>)~ para configurar o shell.

O /downside/  desta solução é  que o passo ~Waiting  for an IP~  acaba demorando
bastante.

*** Usando Docker Swarm para serviços em nuvem

A Docker  Machine aquis  erve apenas para  criar um ambiente  de VMs  locais com
diversas máquinas,  mas pode ser utilizado  também com provedores de  serviço em
nuvem.

Por exemplo, usando o driver  ~amazonec2~, podemos criar diretamente máquinas na
AWS, usando o serviço EC2.

** Criando o swarm

Vamos iniciar o cluster (swarm) a partir da máquina virtual ~vm1~.

#+begin_src bash
docker-machine ssh vm1

# No shell da vm1...
docker swarm init
#+end_src

O segundo comando mostrará um erro porque  não conseguirá selecionar um IP a ser
anunciado, uma vez que o Docker Swarm  tem duas opções: o IP da máquina virtual,
e  o  IP da  máquina  física,  disponível através  de  duas  interfaces de  rede
diferentes.

Por isso,  vamos discriminar o IP  a ser utilizado, o  que na verdade é  uma boa
prática a se fazer.

#+begin_src bash
# Em vm1
docker swarm init --advertise-addr <ip>
#+end_src

No console, será mostrado  que a máquina atual é um ~manager~  (por ter criado o
swarm), e o nó (a máquina) terá também um ID específico.

O console também mostrará um comando para adicionar /workers/ ao swarm.

** Para saber o status da máquina

Para saber se a máquina está em um swarm, use

#+begin_src bash
# Em vm1
docker info
#+end_src

Com um  pouco de  esforço, será possível  ver a linha  ~Swarm: active~  em certo
ponto do log mostrado.

* Responsabilidade dos nós workers

** Criando um worker

Vamos criar mais duas máquinas virtuais.

#+begin_src bash
# No host
docker-machine create -d virtualbox vm2
docker-machine create -d virtualbox vm3
#+end_src

Vamos recuperar o token a ser utilizado para que um worker entre no Swarm.

#+begin_src bash
# Em vm1
docker swarm join-token worker
#+end_src

Agora,  *vá  até  ~vm2~  e  ~vm3~  e cole  o  comando  mostrado  em  ~vm1~  para
adicioná-los*.

** Listando e removendo nós

Para listar os nós do swarm:

#+begin_src bash
# Em vm1
docker node ls
#+end_src

*NOTA:* Apenas um /manager/ pode visualizar ou alterar o estado do cluster.

Para remover um nó  no swarm, o nó precisa estar com  status /Down/. Para tanto,
primeiro precisamos que a ~vm3~, por exemplo, saia do cluster:

#+begin_src bash
# Em vm3
docker swarm leave
#+end_src

Agora, poderemos efetivamente remover o nó do swarm:

#+begin_src bash
# Em vm1
docker node rm <id_da_vm3>
#+end_src

Para  adicionar ~vm3~  novamente,  será  necessário executar  na  mesma o  mesmo
processo anterior de adição através do comando de join com um token específico.

** Subindo um serviço

O comando lembra bastante o ~docker container run~, mas deve ser executado em um
/manager/ porque workers não têm autonomia para mudar o estado do cluster.

Além disso, é possível criar um contêiner em qualquer worker, mas o status desse
contêiner não será anunciado para o  restante das máquinas, portanto não sendo o
ideal para se trabalhar com Swarm.

#+begin_src bash
# Em vm1
docker service create -p 8080:3000 aluracursos/barbearia
#+end_src

Será mostrado em caso de sucesso que o serviço *convergiu*, ou seja, foi criado,
portanto agora há um contêiner sendo executado no escopo do swarm.

*NOTA:* O ~vm1~,  apesar de ser um  /manager/, não está isento,  neste ponto, de
executar os serviços também.

*** Descobrindo o IP de um worker

#+begin_src bash
# Em vm1
docker node inspect vm2
#+end_src

** Tarefas e Routing Mesh

Para listar os serviços:

#+begin_src bash
docker service ls
#+end_src

Pudemos observar ao criar o serviço que foi criada também uma tarefa.

Uma *tarefa* é uma *instância de um serviço* sendo executado.

Para observar as tarefas e onde estão sendo executadas:

#+begin_src bash
docker service ps <id do serviço>
#+end_src

Dessa forma, o  serviço poderá ser acessado  através de acesso direto  à VM (use
~docker  inspect~  no  manager para  ver  o  IP  do  nó  em que  a  tarefa  está
executando), mas também  através do IP do manager (~vm1~)  ou de /qualquer outro
worker!/

Esse redirecionamento é realizado através do processo de /routing mesh/.

Isso também  significa que a  porta ~8080~, usada  pelo serviço em  questão, não
pode ser reutilizada para outros serviços no cluster inteiro.

Caso o  contêiner seja *forçosamente removido*  no nó original, será  criada uma
*nova tarefa* para garantir a persistência do serviço novamente.

* Gerenciando o cluster com managers

Se, por exemplo,  fosse executado ~docker swarm leave --force~  em um manager, o
estado do  swarm (e  consequentemente dos workers)  seria perdido.  Isso poderia
inclusive ser análogo a uma situação de problema de hardware, por exemplo.

Adicionalmente,  note que  os /workers/  não  podem subir  serviços, então  eles
seriam  parcialmente  inutilizados. Todavia,  ainda  seria  possível acessar  os
serviços através do IP dos workers que ainda estão funcionando.

** Fazendo backup do Swarm

Vamos reconfigurar tudo  do zero. Para tanto, executaremos  ~docker swarm leave~
em todos os workers e prosseguiremos com a configuração:

#+begin_src bash
# Em vm1
docker swarm init --advertise-addr <ip>

# Em vm2
docker swarm join --token ... <ip>

# Em vm3
docker swarm join --token ... <ip>

# Em vm1
docker service create -p 8080:3000 aluracursos/barbearia
#+end_src

Para evitar que uma tragédia aconteça,  é possível, por exemplo, realizar backup
direto dos arquivos em ~/var/lib/docker/swarm~, que existe na ~vm1~.

Ao   recuperar   um   backup,   basta   colocar   os   arquivos   novamente   em
~/var/lib/docker/swarm~. Em seguida, execute um comando como o a seguir:

#+begin_src bash
docker swarm init --force-new-cluster --advertise-addr <ip>
#+end_src

** Criando mais managers

Vamos começar refazendo o cluster do zero.

#+begin_src bash
# Em todos os nós
docker swarm leave --force

# Em vm1
docker swarm init --advertise-addr <ip>
#+end_src

Em ~vm1~, recupere o comando para adição de um novo manager:

#+begin_src bash
docker swarm join-token manager
#+end_src

Execute o comando  anterior em ~vm2~ e ~vm3~.  Com isso, as VMs ~1~  a ~3~ agora
são managers.

Vamos adicionar, agora, mais dois workers.

#+begin_src bash
# No host
docker-machine -d virtualbox create vm4
docker-machine -d virtualbox create vm5
#+end_src

Enquanto as VMs são criadas, vamos observar os hosts e seus status na ~vm1~.

#+begin_src bash
docker node ls --format "{{.Hostname}} {{.ManagerStatus}}"
#+end_src

Poderemos observar que ~vm1~ possui status  /Leader/, enquanto ~vm2~ e ~vm3~ são
/Reachable/.

Vamos adicionar ~vm4~ e ~vm5~ como /workers/ do cluster.

#+begin_src bash
# Em vm1
docker swarm join-token worker

# Em vm4 e vm5
docker swarm join --token ... <ip>
#+end_src

*Caso ocorra um  desastre com a ~vm1~,  um dos outros dois  managers será eleito
 Leader*. Mas isso não  é feito de forma arbitrária; é  utilizado o algoritmo de
 consenso RAFT.

** Algoritmo de consenso RAFT

Quando o  Leader falha,  os nós  remanescentes (managers  e workers)  votam para
definir qual deve ser o novo Leader entre os nós managers.

Para que o consenso na votação possa ser atingido:

1. Deve-se suportar um máximo de $\frac{N-1}{2}$ falhas;
2.  Deve   haver  um   mínimo  de  $\frac{N}{2}+1$   de  quórum   de  /managers/
   remanescentes.

(onde $N = $ número inicial de /managers/).

Isso significa que, no caso de  ~vm1~ falhar, suportaremos no máximo, UMA falha,
e precisamos  de um  mínimo de  DOIS managers  remanescentes (neste  caso, temos
exatamente este número).

Adicionalmente, *adicionar muitos  managers não é recomendado*, já  que a adição
de novos  managers impacta  diretamente na  velocidade de  leitura e  escrita no
cluster.

A Docker recomenda sempre 3, 5 ou 7 managers, mas nunca mais que dez -- e sempre
números ímpares, de forma que sempre haverá um consenso.

* Separando as responsabilidades

A responsabilidade do *manager* é orquestrar  o swarm, enquanto os *workers* são
responsáveis  por carregar  e executar  os contêineres.  Todavia, podem  existir
situações em que um *manager* acabe executando serviços também.

A ideia é adicionar restrições para que isso não ocorra.

** Readicionando um manager

Para remover  um nó /manager/  com segurança, primeiro precisamos  rebaixá-lo de
/manager/ para /worker/:

#+begin_src bash
# Caso o nó em vm1 já não tenha sido removido, em vm1:
docker swarm leave --force

# Em vm2
docker node demote vm1
docker node rm vm1
#+end_src

Para readicionar a ~vm1~:

#+begin_src bash
# Em vm2
docker swarm join-token manager

# Em vm1
docker swarm join --token ... <ip>
#+end_src

Vamos subir um serviço...

#+begin_src bash
# Em vm2
docker service create -p 8080:3000 aluracursos/barbearia
#+end_src

Podemos observar  que o  serviço foi  adicionado em ~vm1~,  o que  *não deveria*
ocorrer no nosso caso!

Por  padrão,   o  Docker  Swarm   permite  que  serviços  sejam   executados  em
managers. Queremos que isso não ocorra.

Temos duas formas de evitar que isso ocorra.

*** Removendo todos os serviços

#+begin_src bash
docker service rm $(docker service ls -q)
#+end_src

** Restringindo nós

Para evitar  que um nó não  execute serviços, precisamos trocar  seu /status/ de
disponibilidade:

#+begin_src bash
# Em vm1
docker node update --availability drain vm2
#+end_src

O  comando   anterior  trocará  o   status  de  ~AVAILABILITY~  da   ~vm2~  para
/Drain/. Isso  significa que, caso  haja um serviço  em execução em  ~vm2~ nesse
momento, este serviço será alocado para outro nó.

Esta estratégia tem dois problemas:

1. Isso apenas /muda  o problema de lugar/. O serviço  poderá ser realocado para
   outro nó onde não queremos que  seja executado, e precisaremos mudar o status
   de disponibilidade do outro nó também, e assim por diante.
2. A  ideia é que  os managers  não executem o  serviço de ~barbearia~,  mas que
   ainda sejam capazes de executar certos serviços. Se um nó tem disponibilidade
   /Drain/, então nenhum serviço pode ser executado no mesmo.

** Restringindo serviços

Esta  possibilidade  é   menos  invasiva  que  a   anterior.  Nessa  estratégia,
restringimos o serviço para que ele  não seja executado em determinados tipos de
nós.

Antes de mais nada, execute o serviço caso já não esteja em execução:

#+begin_src bash
# Em vm1
docker service create -p 8080:3000 aluracursos/barbearia
#+end_src

De  forma  análoga   ao  gerenciamento  de  nós,  podemos   atualizar  dados  de
serviços. Neste caso, vamos adicionar uma /constraint/ ao serviço criado:

#+begin_src bash
# Em vm1
docker service update --constraint-add node.role==worker <id_do_serviço>
#+end_src

Caso fosse  aplicada alguma  /constraint/ impossível de  ser atendida,  o Docker
atualizaria o serviço, mas avisaria que não há nenhum nó capaz de atender àquela
/constraint/. E por consequência, não  haveria mais tarefas para aquele serviço,
porque nenhum nó poderia atender àquela  exigência. Assim, o serviço não estaria
mais acessível.

*** Outras restrições

Poderíamos usar também /constraints/ usando propriedades como ~id~ e ~hostname~,
além de ~role~. Eis alguns exemplos.

#+begin_src bash
# Restringir o serviço para rodar em um nó de ID específico
docker service update --constraint-add node.id==<id> <id_do_serviço>

# Restringir o serviço para rodar apenas na vm4:
docker service update --constraint-add node.hostname==vm4 <id_do_serviço>
#+end_src

*** Removendo restrições

Podemos também remover  explicitamente /constraints/ de um  serviço. Para tanto,
basta trocar a flag ~--constraint-add~ por ~--constraint-rm~.

#+begin_src bash
docker service update --constraint-rm node.hostname==vm4 <id_do_serviço>
#+end_src

* Serviços globais e replicados

*Nós managers também podem agir como nós de tipo /workers/*.

Alguns  serviços ainda  podem  rodar  em nós  de  tipo /manager/,  especialmente
serviços de:

- Monitoramento;
- Segurança;
- Serviços críticos em geral.

** Serviços replicados

Vamos começar removendo a constraint anterior  e então olhando para o serviço em
execução.

#+begin_src bash
# Em vm1
docker service update --constraint-rm node.role==worker <id_do_serviço>
docker service ls
#+end_src

#+begin_src text
ID                  NAME                MODE                REPLICAS            IMAGE                          PORTS
c6gcw38fhcms        eloquent_mccarthy   replicated          1/1                 aluracursos/barbearia:latest   *:8080->3000/tcp
#+end_src

Podemos  observar que  o  serviço possui  modo ~replicated~  e  possu uma  única
réplica.

O Docker Swarm  possui um modo /replicado/  e um modo /global/.  O modo /global/
envolve rodar um serviço em todos os nós, e será visto mais adiante.

O modo  /replicado/ é  o tipo  padrão de um  serviço Swarm,  e opera  através da
criação de  várias tarefas para  um mesmo serviço,  para que outra  tarefa possa
responder a requisições, quando uma tarefa em específico ficar muito ocupada.

Para aumentar o número de réplicas desse serviço:

#+begin_src bash
# Em vm1
docker service update --replicas 4 <id_do_serviço>
#+end_src

Podemos observar agora como o serviço está operando através de várias tarefas:

#+begin_src bash
docker service ps <id_do_serviço>
#+end_src

*** Comando ~service scale~

Outra forma de escalar um serviço é usando o comando ~scale~:

#+begin_src bash
docker service scale <id_do_serviço>=<numero_de_replicas>
#+end_src

Exemplo:

#+begin_src bash
# Em vm1
docker service scale c6=5
#+end_src

** Serviços globais

Vamos começar recriando o serviço em modo global.

#+begin_src bash
# Em vm1
docker service rm <id_do_serviço>
docker service create -p 8080:3000 --mode global aluracursos/barbearia
#+end_src

Note  a  flag  ~--mode  global~,  que substitui  o  padrão  ~--mode  replicated~
(utilizado também quando o modo não é informado).

Serão  criadas, neste  caso,  cinco  tarefas deste  serviço,  e  o serviço  será
executado em cada nó.

Serviços de  monitoramento e segurança  são exemplos  de serviços que  podem ser
executados de forma global.


